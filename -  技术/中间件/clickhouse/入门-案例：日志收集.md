
## 一、表结构设计与核心配置

  - 

### 1. 基础表结构核心字段

  

针对`eas.sdk_error_logs`错误日志表，核心字段需覆盖**唯一标识、时间维度、业务维度、错误维度**，关键字段如下：

  
  
  

* 唯一标识：`id`（16 位唯一码）、`request_id`（请求标识）、`md5_stacks`（错误堆栈 MD5，用于聚合相同错误）

  

* 时间维度：`ts`（客户端上报时间戳）、`create_time`（数据写入时间）、`date`（日期字符串，冗余字段）

  

* 业务维度：`project`（项目标识）、`os`/`os_version`/`os_name`（设备系统）、`version`/`version_code`（APP 版本）、`package_name`（APP 包名）

  

* 错误维度：`error_type`（错误类型）、`error_reason`（错误原因）、`error_stacks`（错误堆栈）、`crash_count`（崩溃次数）

  

### 2. 分区键（PARTITION BY）设计

  
  
  

* **核心选择**：按时间分区，使用`toYYYYMMDD(ts)`（按`ts`的日期分区，如`20230901`）

  

* **作用**：

  

1. 减少查询扫描范围（仅扫描目标日期分区）；

  

2. TTL 过期时直接删除整个分区，效率更高；

  

* **旧版本兼容**：若`ts`是毫秒级`UInt64`，需先转秒级再分区：`toYYYYMMDD(toDateTime(ts/1000))`（ClickHouse 20.3 不支持`fromUnixTimestamp`）。

  

### 3. 排序键（ORDER BY）设计

  
  
  

* **用户表配置**：`(project, os, ts, version, package_name, error_type, error_reason)`

  

* **设计原则**：

  

1. 优先放高频过滤字段（如`project`、`os`），利用一级稀疏索引快速定位；

  

2. 时间字段`ts`紧跟，适配按 “项目 + 系统 + 时间” 的常见查询场景；

  

3. 避免冗余字段，防止排序效率下降。

  

### 4. 核心参数（SETTINGS）

  
  
  

* `index_granularity = 8192`：一级稀疏索引的粒度（每 8192 行生成一个索引项），默认值适配大多数日志场景，无需修改。

  

## 二、索引设计实践（针对日志高频查询）

  

### 1. 一级索引（默认生成）

  
  
  

* **原理**：`MergeTree`引擎自动为`ORDER BY`字段生成**稀疏索引**，非全量索引，按`index_granularity`间隔存储索引项；

  

* **作用**：快速定位 “项目 + 系统 + 时间” 等`ORDER BY`字段组合的查询范围，避免全表扫描。

  

### 2. 二级索引（自定义，针对高频查询字段）

  

用户高频查询场景：按`request_id`（请求标识）或`md5_stacks`（错误堆栈 MD5）查询，需添加**set 类型二级索引**（适合离散值、等值查询）。

  

#### （1）索引创建语法（修正用户原表错误）

  
  
  

```

\-- 针对md5\_stacks的二级索引（哈希集合，快速判断值存在）

  

INDEX idx\_md5\_stacks (md5\_stacks) TYPE set(1024) GRANULARITY 4,

  

\-- 针对request\_id的二级索引

  

INDEX idx\_request\_id (request\_id) TYPE set(1024) GRANULARITY 4

```

  

#### （2）关键参数说明

  
  
  

* `TYPE set(1024)`：`set`类型适合高基数离散值，`1024`是哈希桶数量（根据字段基数调整，基数高则增大）；

  

* `GRANULARITY 4`：索引粒度（每 4×8192=32768 行生成一个索引项），平衡索引存储开销与查询效率（基数高用 1-4，基数低用 8）。

  

#### （3）其他索引类型适配场景

  
  
  

* 若需模糊查询（如错误堆栈关键词）：用`ngrambf_v1`（基于 N-gram 布隆过滤器）；

  

* 若字段基数低（如`os`只有 Android/iOS）：用`minmax`类型（存储粒度内最值，快速过滤）。

  

## 三、自增字段实现（CK 无原生自增）

  

CK 不支持 MySQL 式`AUTO_INCREMENT`，需通过以下方式模拟，解决 “重排打乱原始顺序” 问题：

  

### 1. 程序端生成自增 ID（推荐）

  
  
  

* 逻辑：插入前查询表中当前最大 ID，加 1 作为新 ID；

  

* 示例（Python）：

  
  
  

```

import clickhouse\_connect

  

client = clickhouse\_connect.get\_client(...)

  

\# 获取当前最大id

  

max\_id = client.query("SELECT max(toUInt64(id)) FROM eas.sdk\_error\_logs").result\_set\[0]\[0] or 0

  

new\_id = str(max\_id + 1).zfill(16) # 补全16位

  

\# 插入数据

  

client.insert("eas.sdk\_error\_logs", \[\[new\_id, ...]])

```

  

### 2. 查询时生成临时序号（仅展示用）

  

若无需存储自增 ID，仅查询时展示顺序，用`RowNumberInAllBlocks()`：

  
  
  

```

SELECT&#x20;

  

&#x20; RowNumberInAllBlocks() AS row\_num, -- 临时自增序号

  

&#x20; id, ts, error\_type&#x20;

  

FROM eas.sdk\_error\_logs&#x20;

  

WHERE project = 'app\_payment'&#x20;

  

ORDER BY ts;

```

  

## 四、时间类型与精度处理（含旧版本兼容）

  

### 1. 时间类型选择

  
  
  

| 类型 | 精度 | 适用场景 | 旧版本（20.3）兼容性 |

| ------------ | ---------- | --------------- | --------------------- |

| `DateTime` | 秒级 | 无需毫秒精度的日志 | 完全兼容（支持 TTL、分区） |

| `DateTime64` | 毫秒 / 微秒 | 需要高精度时间（如崩溃时间） | 不兼容 TTL（需转`DateTime`） |

| `UInt64` | 毫秒级（存储时间戳） | 需兼容旧版本 + 保留毫秒精度 | 完全兼容（推荐） |

  

### 2. 毫秒级时间存储方案（ClickHouse 20.3 兼容）

  

由于 20.3 版本不支持`DateTime64`的 TTL 和`fromUnixTimestamp64Milli`，推荐用`UInt64`存储**毫秒级时间戳**（如 13 位数字`1693535400123`），转换逻辑如下：

  
  
  

* 分区转换：`toYYYYMMDD(toDateTime(ts/1000))`（毫秒转秒级，再转`DateTime`）；

  

* TTL 转换：`toDateTime(ts/1000) + toIntervalHour(2160)`（先转`DateTime`再算过期）；

  

* 查询转换：`toDateTime(ts/1000)`（转成可读时间）。

  

### 3. 旧版本函数兼容问题

  
  
  

* 问题 1：20.3 无`fromUnixTimestamp`，用`toDateTime(秒级时间戳)`替代；

  

* 问题 2：20.3 无`fromUnixTimestamp64Milli`，用`ts/1000`转秒级后再用`toDateTime`；

  

* 示例：

  
  
  

```

\-- 毫秒时间戳（UInt64）转可读时间

  

SELECT toDateTime(1693535400123 / 1000) AS ts\_readable; -- 结果：2023-09-01 10:30:00

```

  

## 五、TTL 配置（自动清理旧日志）

  

### 1. TTL 核心语法（用户场景：保留 90 天日志）

  
  
  

```

TTL toDateTime(ts/1000) + toIntervalHour(2160) -- 2160小时=90天

```

  

### 2. 关键说明

  
  
  

* **TTL 原理**：以`ts`（毫秒级`UInt64`）转成的`DateTime`为基准，加 90 天，过期数据后台合并时自动删除；

  

* **旧版本兼容**：TTL 仅支持`DateTime`/`Date`类型，`DateTime64`需先转`toDateTime(ts/1000)`；

  

* **注意事项**：TTL 非实时删除，依赖后台合并任务，过期数据可能延迟 1-2 天清理。

  

## 六、时区问题与避免错误

  

### 1. 是否需要设置时区？

  
  
  

* **推荐设置场景**：跨时区业务（如日志来自国内外设备）、分布式集群（节点时区可能不同）；

  

* **无需设置场景**：单一时区业务（如仅国内，服务器默认`Asia/Shanghai`）。

  

### 2. 避免时区错误的实践方案

  

#### （1）旧版本（20.3，无函数内时区参数）

  
  
  

* 方案 1：统一存储**UTC 毫秒时间戳**（客户端上报时转 UTC，避免时区偏差），查询时在应用层转业务时区（如东八区加 8 小时）；

  

* 方案 2：固定服务器时区，修改`config.xml`：`<timezone>Asia/Shanghai</timezone>`（需重启服务），确保`toDateTime`转换基于业务时区。

  

#### （2）新版本（支持函数内时区参数）

  

显式指定时区，彻底脱离默认配置：

  
  
  

```

\-- 毫秒时间戳转东八区时间

  

SELECT toDateTime(ts/1000, 'Asia/Shanghai') AS ts\_cst

  

FROM eas.sdk\_error\_logs;

```

  

#### （3）核心原则

  
  
  

1. 输入统一：客户端上报时间需明确时区（如转 UTC 或东八区时间戳）；

  

2. 存储原始：优先用`UInt64`存时间戳，避免写入时绑定时区；

  

3. 输出明确：查询时显式转换为业务时区（如用`convert_tz`函数）。

  

## 七、查询优化原理与验证

  

### 1. 查询优化逻辑（以用户高频查询为例）

  

#### 场景：按`project`+`os`+`md5_stacks`查询

  
  
  

```

SELECT id, error\_stacks, ts&#x20;

  

FROM eas.sdk\_error\_logs&#x20;

  

WHERE project = 'app\_payment'&#x20;

  

&#x20; AND os = 'Android'&#x20;

  

&#x20; AND md5\_stacks = 'a1b2c3d4e5f6';

```

  

优化步骤：

  
  
  

1. **分区裁剪**：按`ts`对应的`toYYYYMMDD(toDateTime(ts/1000))`过滤目标分区；

  

2. **一级索引定位**：用`ORDER BY`字段`(project, os)`定位到该组合的数据范围；

  

3. **二级索引过滤**：用`idx_md5_stacks`排除不包含目标 MD5 值的粒度块；

  

4. **扫描数据块**：仅扫描最终筛选后的小范围数据块，返回结果。

  

### 2. 索引命中验证

  

用`EXPLAIN INDEXES=1`查看是否命中索引：

  
  
  

```

EXPLAIN INDEXES=1

  

SELECT id FROM eas.sdk\_error\_logs WHERE md5\_stacks = 'a1b2c3d4e5f6';

```

  
  
  

* 命中标识：结果中出现`Using index idx_md5_stacks`，表示二级索引生效。

  

> （注：文档部分内容可能由 AI 生成）