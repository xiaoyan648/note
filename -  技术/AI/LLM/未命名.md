# 上文总结


## AI如何火起来的

### 一、Transformer到底是啥？

Transformer的核心，就是**能同时看懂一句话里所有字的关系**。比如看“他用电脑写代码”，它能一下子搞明白“他”和“写”是主仆关系，“电脑”是“写”的工具——不用像以前那样逐字逐句琢磨。

### 二、AI为啥现在才火？

- 以前的AI(比如RNN)，**只能顺着顺序读字**，读后面忘前面。比如读长篇小说，读到第100章就忘了第1章的内容，根本理解不了整体剧情。
    
- 现在有了Transformer，再加上**网上的海量数据**(就像给它喂了全世界的书)，它不光能记住前文，还能从数据里自己总结规律，比如“看到‘下雨’就联想到‘打伞’”。而且**它处理数据超级快**，能同时分析百万字内容，这才让AI有了实用价值。
    
- 举例：要整理一排书：《西游记》《红楼梦》《三国演义》《水浒传》，目标是搞清楚《红楼梦》和其他三本书的关联程度。
    
    - 之前的RNN/LSTM等循环神经网络技术：就像一个**只能按顺序看书的人**，一步一步来，花4个月完成任务。
        
        暂时无法在飞书文档外展示此内容
        
    - Transformer就像一个**站在书架前的人**，能一眼看到所有书，同时计算关联，一周搞定。
        
        暂时无法在飞书文档外展示此内容
        

### 三、谷歌和OpenAI干了啥？

- **谷歌**：2017年发明了Transformer，相当于造出了AI的“聪明大脑”。
    
- **OpenAI**：把这个大脑用海量数据喂饱，从GPT-1到ChatGPT，数据量翻了上万倍，让AI从“只会填空”变成“啥都能聊”，直接火遍全球。
    
- 为啥Google发明了Transformer，确没有第一个火呢？
    
    - 最初产品定位不同，当时主要想搞google翻译和核心业务搜索排序(后面也确实在这两个业务上先用的)。
        
    - 算力ROI不一定划算，就算Google也不是GPU完全充裕的，再采买更多，ROI不一定合理，像国内只有字节一家有完全充足的GPU，阿里/腾讯都是从字节采购和租赁。
        
    - 人才流失：6位作者全离职、2位没有加入google：创业 + 加入OpenAI
        
        暂时无法在飞书文档外展示此内容
        

### 四、为啥以前的AI没火？

以前的RNN就像**脑子不好使的学生**：

- 读东西慢，百万字要算几小时；
    
- 记不住前文，长文本理解能力差；
    
- 没足够数据喂它，就算想学习也没材料。所以只能做些简单的文字识别，成不了气候。
    

## 有几个预判(我个人的)：

- 对话式的应用越来约日常化，助理化，语音化(市场可以更下沉)；
    
- 工业场景应用全面Agent化，不再是对话式；
    
- 工业场景的应用对实效和延迟越来越敏感；
    
- 知识工程占据未来核心位置，而不是算力。
    

# 本次内容：从海量数据中训练出通用模型

## 整体流程

![[IMG-20260114201235468.png]]

一、**学习类型：无监督、自监督、有监督、半监督**

|   |   |   |
|---|---|---|
|**学习类型**|**例子：教小孩儿认苹果**|**核心特点**|
|**有监督**|你指着苹果说：“这是苹果，红色、圆的”。小孩学 “特征→名字” 的对应|有**标准答案**（人工标注）|
|**无监督**|你扔一堆水果（苹果、香蕉、橘子）给小孩，让他自己分堆。小孩靠外形颜色分成三堆|无**标准答案**，自己找规律|
|**自监督**<br><br>(Transformer)|你把苹果的 “红色” 盖住，让小孩猜被盖的部分是什么。小孩靠 “圆的→红色” 的常识猜|用**数据本身做标签**，不用人标|
|**半监督**|你只告诉小孩 “这 1 个是苹果”，再给他 100 个水果，让他找出剩下的苹果|少量**标注数据** + 大量**无标注数据**|

二、用一句话总结：海量数据炼基座，SFT+RLHF调成通用大模型

三、核心两步：先炼**通用底子****，**再**调成好用的工具**

- **第一步：****海量数据****炼出基座模型**把全世界各类数据(文本、对话、知识、代码等)喂给Transformer，让它反复做“填空题”——遮住内容里的字词，逼它根据上下文猜。Transformer靠自注意力机制，**摸清语言逻辑和知识关联**(比如“感冒”关联“发烧”“多喝水”)。练完后，AI有了“啥都懂点”的基础，但回答生硬、不贴合人类需求，这就是**基座模型**。
    
- **第二步：**需两轮调教升级成通用大模型，基座模型是“毛坯”，需两道工序精加工：
    
    - **SFT(监督微调 Supervised Fine-Tuning)**：喂“问题→标准答案”的标注数据，教AI按任务需求输出(比如“写请假条→给规范模板”)，让AI学会“干活”。
        
    - **RLHF(****基于人类反馈的强化学习 Reinforcement Learning From Human Feedback****)**：让AI对同一问题生成多个回答，人类打分选最优；用打分结果训练奖励模型，引导AI越练越懂人类喜好，回答更自然贴心。两步完成，基座模型就升级成**能听懂人话、会办人事的通用大模型**。
        

## 训练流程

1. ### 什么是LLM基座模型？
    

基座模型就是**AI的“基础版大脑”**，是个啥都懂点但还不会灵活干活的“毛坯”。它学了海量通用知识和语言规律，能看懂人话、有基本逻辑，但回答生硬，没法直接满足具体需求(比如写文案、做问答)。

2. ### 为什么基座模型需要预训练？预训练做了什么？
    

➡️ 为什么：

- 因为AI天生“啥也不会”，预训练就是**给它打通用基础**。不预训练的话，模型就是个空壳，连基本的语言逻辑、常识都没有，后续没法调教。
    

➡️ 做了什么：

- 喂**海量通用数据**：覆盖面广、无明显偏向，目的就是让模型学通用规律，而不是某一领域的专属内容
    
- 让模型反复做“填空题”，自己总结**语言规律和知识关联**(比如“吃饭”对应“筷子”)。
    

3. ### 为什么不能直接用预训练好的基座模型？为什么还需要后续步骤？
    

基座模型就是“毛坯”——回答生硬、答非所问，还会“胡说”；不懂具体任务需求，也摸不透人类喜好，必须靠后续步骤打磨成“成品”。

4. ### 预训练之后做什么？基座模型如何变成有用模型？
    
      预训练后走两步核心流程：
    
    - **SFT(监督微调)**：用“问题→标准答案”数据，教模型**按任务干活**；
        
    - **RLHF(人类反馈强化学习)**：靠人类打分优化，让模型**懂人类喜好**；
        
    - 可选优化：加RAG、轻量化等，适配实际使用场景。
        
    
      最终：**基座模型****→SFT→RLHF→优化=有用模型**
    

### 5.如何训练？
![[IMG-20260114201315127.png]]    

**`5.1数据准备与处理：产出模型可用的数据`**

- ➡️海量数据
    
    暂时无法在飞书文档外展示此内容
    
- ➡️token化：成本低、灵活、适配性强
    
    - **第一步：**句子拆成单个字+标点，→`今 天 来 上 海 出 差 了 。 我 在 外 滩 喝 咖 啡 。 这 里 的 风 景 很 美 。`
        
    - **第二步：高频组合合并**统计这些字相邻出现的次数，反复合并高频组合：
        
        - 统计后发现**“上海”“外滩”“咖啡”“风景”**凑一起的次数最多，先把它们合并成新块→合并后变成`今 天 来 上海 出 差 了 。 我 在 外滩 喝 咖啡 。 这 里 的 风景 很 美 。`
            
        - 再统计一轮，若**“今天”“这里”**是高频组合，继续合并→最终变成`今天 来 上海 出 差 了 。 我 在 外滩 喝 咖啡 。 这里 的 风景 很 美 。`
            
        - 给这些块贴唯一数字标签(比如`今天=1`、`上海=2`……)，就成了模型能看懂的**token**
            

**`5.2预训练`**

  使用很多机器，就像**100个工人一起盖大楼**，分工明确、效率翻倍。假设我们有**1亿条文本数据**、**10台机器**，流程如下：

- **第一步：先把1亿条数据全做成token**用BPE把1亿条文本全拆成带数字标签的token序列，再打乱顺序，避免模型学偏。
    
- **第二步：切分数据，分给10台机器**
    
    - 把1亿条token数据切成**10万个小批次**(每批1000条token)；
        
    - 主机器给9台从机器**各分1万个批次**，自己也留1万个批次，每台机器拿到的任务量一样。
        
- **第三步：10台机器同时“刷题”**每台机器上的Transformer模型，学习**文本中各元素的关联规律，**比如处想大量的"上海外滩"、"上海的外滩"、"外滩是上海的知名地点"等信息，就学到外滩是上海的，有关联。
    
- **第四步：反复循环，直到学完所有数据**重复“分批次→并行算→汇总更参数”，把10万个批次的数据全刷完，模型就学好了。
    

  得到**基座模型**，不能直接用，回答生硬、答非所问，还会“胡说”；不懂具体任务需求，也摸不透人类喜。

  

**`6.4微调与对齐，`**`监督微调(SFT)和强化学习(这里指RLHF)`**`不是给模型 “塞新知识”，`**`而是` **`“教模型怎么用知识”`**

- **监督微调(SFT)**：高质量指令数据(人工编写或合成)，使模型适应具体任务(如对话、编程)，教模型照葫芦画瓢。
    
    - **训练数据：**问题+标准答案
        
        |   |   |   |
        |---|---|---|
        |用户指令|标注答案1|标注答案2|
        |成都必去的地方是啥|大熊猫繁殖基地|熊猫基地|
        
    - 学习：SFT的作用是**教模型“能回答”**，只告诉模型“这个问题可以这么答、也可以那么答”，但没教它“哪种情况该选哪个”。
        
        - 人类需求的核心：问成都必去景点，别答非所问，要给和熊猫基地相关的答案。
            
        - SFT的做法：喂给模型“指令-答案”配对数据(比如“成都必去的地方是啥”→“大熊猫繁殖基地/熊猫基地”)。
            
        - 对齐结果：模型学会“这个问题只能答熊猫基地相关内容”，不会乱答“长城”“西湖”，先满足人类“不跑偏”的基本需求。
            
- **人类反馈强化学习(RLHF**ReinforcementLearningfromHumanFeedback，基于人类反馈的强化学习**)**：基于人工排序数据训**奖励建模**，通过**PPO**(ProximalPolicyOptimization-近端策略优化)**对齐人类偏好**
    
    - **训练数据：人类偏好打分**针对**朋友闲聊、日常推荐**的场景，给两个答案打分(满分5分)：
        
    
    |   |   |   |
    |---|---|---|
    |答案|分数|打分理由|
    |大熊猫繁殖基地|2分|太正式，聊天说起来绕口|
    |熊猫基地|5分|简称顺口，符合口语习惯|
    
    - **模型学习偏好**模型通过打分数据学到：**口语场景下，回答“熊猫基地”更讨喜**。
        
        - 人类需求的延伸：不同场景想要不同答案——和朋友闲聊要口语化的“熊猫基地”，写攻略要正式的“大熊猫繁殖基地”。
            
        - RLHF的做法：
            
            - 人类标注员按场景给SFT的两个答案打分(口语场景：“熊猫基地”5分，“大熊猫繁殖基地”2分)；
                
            - 让模型学习“分数和场景的对应关系”。
                
        - 对齐结果：模型学会“看语境输出”——用户口语提问就答“熊猫基地”，正式提问就答“大熊猫繁殖基地”，满足人类“合心意”的个性化需求。
            
- **举例**：SFT和RLHF本质：针对性**调整模型里“词与词的关联权重”**，让模型在特定提示词场景下，优先触发符合人类需求的关联，最终输出更优答案。用**“成都必去景点”**这个场景，详述**词权重调整**的过程：
    
    ![[IMG-20260114201341731.png]]
    
    - 前提：基座模型的“原始权重”很随机，基座模型预训练后，已经记住了很多词的关联，但权重很平均，没有针对性。比如看到提示词**“成都必去景点”**，
        
        - 模型里的词关联权重是这样的：
            
            - “成都”↔“熊猫基地”：权重0.3
                
            - “成都”↔“宽窄巷子”：权重0.3
                
            - “成都”↔“火锅”：权重0.2
                
            - “成都”↔“长城”：权重0.05(很低，但没完全消失)
                
        - 这时候你问模型“成都必去景点”，它可能乱答——有时说熊猫基地，有时说宽窄巷子，甚至偶尔扯到火锅。
            
    - 监督微调(SFT)：**锁定核心关联，抬高目标词权重，给模型“划重点”**，通过“指令-答案”配对数据，强制让模型抬高“问题词”和“答案词”的关联权重。
        
        - **训练数据**：“成都必去景点”→“熊猫基地/大熊猫繁殖基地”
            
        - **权重调整过程**：
            
            - 模型每次看到“成都必去景点”，就尝试输出答案，一开始可能输出“宽窄巷子”；
                
            - 系统告诉它“错了，应该优先输出熊猫基地相关”；
                
            - 模型就**主动调高“成都必去景点”和“熊猫基地”的关联权重**，同时压低和“宽窄巷子”“火锅”的权重。
                
        - **调整后权重**：
            
            - “成都必去景点”↔“熊猫基地”：权重0.8
                
            - “成都必去景点”↔“宽窄巷子”：权重0.1
                
            - “成都必去景点”↔“长城”：权重0.01(几乎清零)
                
        - **能力加强效果**：再问“成都必去景点”，模型**100%会答熊猫基地相关**，不会跑偏——这就是靠**强化核心词的关联权重**实现的。
            
    - **人类反馈强化学习**(RLHF)：**细分场景权重，让答案适配偏好，**经过SFT，模型已经会答熊猫基地了，但分不清“该说全称还是简称”。RLHF就是**按场景给权重“再分层”**。
        
        - **第一步：人类打分定偏好**
            
            - 口语场景提示词：“成都有啥必去的地方啊”→答“熊猫基地”得5分，答“大熊猫繁殖基地”得2分；
                
            - 正式场景提示词：“请推荐成都必去景点(书面报告用)”→答“大熊猫繁殖基地”得5分，答“熊猫基地”得2分。
                
        - **第二步：模型调整“场景词+答案词”的权重**：
            
            - 模型学到“口语提示词(有啥、啊)”和“熊猫基地”的关联要更高；
                
            - 同时学到“正式提示词(请推荐、书面报告)”和“大熊猫繁殖基地”的关联要更高。
                
            - **最终权重分层**：
                
                |   |   |   |
                |---|---|---|
                |提示词类型|关联词|权重|
                |口语型(成都有啥必去的)|熊猫基地|0.9|
                |口语型(成都有啥必去的)|大熊猫繁殖基地|0.2|
                |正式型(书面推荐成都景点)|大熊猫繁殖基地|0.9|
                |正式型(书面推荐成都景点)|熊猫基地|0.2|
                
        - **能力加强效果**：模型能根据提示词的语气，自动选高权重的答案——口语问就说“熊猫基地”，正式问就说“大熊猫繁殖基地”。
            

**`6.5 各大公司的AI“性格”，`**`从一些白皮书、博客、外部演讲的信息、实际使用看，其AI“性格”不同`

![[Pasted image 20260114201358.png]]