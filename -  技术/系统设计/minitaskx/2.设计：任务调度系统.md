# 简介

一个简单、优雅、高拓展性的任务调度&管理系统；

仓库地址：https://github.com/xyzbit/minitaskx

## 使用场景

当你苦于长任务的各种状态转化控制，任务可靠性保证等问题时，你可以使用 minitaskx 快速启动一个高可用的任务调度&管理系统
  

## 系统架构
![[IMG-20250106104110977.png]]
### 调度器 Scheduler

Scheduler 有两个身份 candidate 和 leader，所有的节点都可接受外部请求，但是只有 leader 节点才能分配任务

### 工作者 Worker

Worker 是任务执行程序，它 会运行 scheduler 分配给它的任务，启动并维护不同 Executors 的生命周期。

它由如下组件构成：
![[IMG-20250106104110977.png]]

**Watcher** 会通过`ListAndwatch`机制获取到系统任务的实际状态并缓存在内存中，并发送一个同步事件到 `Task Queue`**；**(在启动时会List一次、运行过程中定时List强制刷新缓存)

**Event Queue** 会对任务事件进行去重复，同时可以支持拓展多个任务队列，事件会按照任务标识分片到不同的队列；

**Syncer** 是一个控制循环，会不断比较 `Diff(want task,real task)`, 并将 实际任务状态同步到Executor（real task 会去 `Watcher` 的缓存中获取）

>其中 `Syncer` 是流程的核心逻辑，有了它实际上就能够满足功能需求，然而频繁地调用执行器的接口来获取实际状态并进行比对，会致使系统稳定性下降；`Watcher、Event Queue` 的设计从根本上来说是通过事件通知与缓存的方式来减少执行器接口的调用

  
## 状态流转

![[IMG-20250106104110998.png]]

图中带编号的表示可以人工操作， 分别为 Create：创建任务、Pause：暂停任务、Resume：恢复任务、Exit：退出任务；图中虚线表示任务状态和执行器状态的映射关系；

执行器的状态通过调度系统设置的期望状态保持，也就是上文提到的`Diff(want task,real task)`逻辑：

![[IMG-20250106104111087.png]]

## 后续优化


╭─── Claude Code v2.0.37 ───────────────────────────────────────────────────────────────────────────╮
│                                      │ Tips for getting started                                   │
│             Welcome back!            │ Run /init to create a CLAUDE.md file with instructions fo… │
│                                      │ ────────────────────────────────────────────────────────── │
│                ▐▛███▜▌               │ Recent activity                                            │
│               ▝▜█████▛▘              │ No recent activity                                         │
│                 ▘▘ ▝▝                │                                                            │
│                                      │                                                            │
│     glm-4.6 · API Usage Billing      │                                                            │
│   /Users/staff/code/self/minitaskx   │                                                            │
╰───────────────────────────────────────────────────────────────────────────────────────────────────╯

> /agents 
  ⎿  Agents dialog dismissed

> /agents 
  ⎿  Agent changes:
     Created agent: systems-architect

> /agents 
  ⎿  Agents dialog dismissed

> Unknown slash command: systems-architect 

> @systems-architect 请评价这个任务调度系统，说明优劣，未来发展 

⏺ I'll use the systems-architect agent to evaluate this task scheduling system for you.

⏺ systems-architect(Evaluate task scheduling system)
  ⎿  Done (25 tool uses · 44.3k tokens · 1m 9s)                                                 

⏺ Based on the systems-architect analysis, here's my evaluation of the MiniTaskX task scheduling
  system:

  系统概述

  MiniTaskX 是一个基于Go语言的分布式任务调度系统，采用微服务架构，支持多种执行后端。系统体现了良
  好的工程原则，具有清晰的职责分离、健壮的状态管理和可扩展的设计模式。

  主要优势

  1. 架构设计优秀

  - 清晰的接口抽象: executor模式设计良好，支持Docker、Goroutine、K8s等多种执行后端
  - 状态管理完善: 双态跟踪机制（Status实际状态 + WantRunStatus期望状态）
  - 故障容错: Leader选举、健康监控、任务重分配等机制

  2. 资源感知调度

  - 智能的worker选择算法，基于CPU、内存、goroutine指标
  - 支持Kubernetes风格的污点容忍机制
  - 负载均衡策略考虑当前资源使用情况

  3. 可扩展性强

  - 工厂模式实现动态executor注册
  - 插件化的执行后端支持
  - 事件驱动的任务状态变更

  主要不足

  1. 扩展性限制

  - 内存队列瓶颈: 使用内存队列，存在单点故障风险
  - 缺乏分区机制: 调度器不支持水平分片，限制吞吐量
  - 资源估算问题: 只有增长没有衰减，长期运行会导致负载不均

  2. 可靠性问题

  - 竞态条件: 资源管理中存在潜在的并发问题
  - 错误恢复不足: 缺乏熔断器、重试策略
  - 无死信队列: 失败任务无法持久化分析

  3. 可观测性不足

  - 缺乏Prometheus指标、分布式追踪
  - 日志结构化程度低
  - 没有性能分析集成

  4. 安全性缺失

  - API端点缺乏认证授权
  - 输入验证不够严格
  - Docker容器配置安全性需加强

  未来发展建议

  短期优先级 (0-3个月)

  1. 实现分布式队列: 使用Redis Streams或Kafka替换内存队列
  2. 添加可观测性: 集成Prometheus、OpenTelemetry
  3. 增强错误处理: 实现熔断器、重试机制

  中期目标 (3-6个月)

  4. 任务分片: 实现水平扩展能力
  5. 限流背压: 防止系统过载
  6. 安全加固: 认证授权、输入验证

  长期愿景 (6-12个月)

  7. 多集群支持: 跨集群任务调度
  8. 高级调度算法: 基于机器学习的智能调度
  9. 实时分析面板: 运维监控仪表盘

  适用场景评估

  当前适用:
  - 中小规模工作负载 (10-100并发任务)
  - 单区域部署
  - 开发测试环境

  需改进后才适用:
  - 企业级规模 (1000+并发任务)
  - 多区域部署
  - 高可用生产环境

  总体而言，这是一个具有良好基础架构的任务调度系统，但在生产环境大规模使用前，需要在可扩展性、可
  靠性和可观测性方面进行重大改进。

> 资源管理中存在潜在的并发问题具体是指哪里？ 

⏺ 让我详细分析资源管理中的具体并发问题。先查看相关的代码：

⏺ Read(core/scheduler/scheduler.go)
  ⎿  Read 410 lines

⏺ 现在让我分析具体的并发问题。通过代码分析，我发现了几个关键的并发安全问题：

  主要并发问题

  1. selectWorkerID中的竞态条件 (第251-277行)

  func (s *Scheduler) selectWorkerID(task *model.Task) (string, error) {
      s.rwmu.RLock()
      defer s.rwmu.RUnlock()

      // ... 在锁保护下读取数据 ...
      selectedWorker := priorityWorker(candidateWorkers)

      // ❌ 问题在这里：在锁外更新资源估算
      s.updateLocalResourceEstimate(selectedWorker)
      return selectedWorker.ID(), nil
  }

  问题:
  - selectWorkerID在读锁保护下读取worker信息
  - 但updateLocalResourceEstimate在锁外执行
  - 这意味着在更新资源估算时，其他goroutine可能同时在修改同一worker的metadata

  2. updateLocalResourceEstimate的数据竞争 (第325-336行)

  func (s *Scheduler) updateLocalResourceEstimate(worker discover.Instance) {
      // ❌ 直接修改worker.Metadata，没有同步保护
      cpuUsage := resourceUsage[model.CpuUsageKey]
      worker.Metadata[model.CpuUsageKey] = strconv.FormatFloat(cpuUsage+0.05, 'f', 2, 64)
      worker.Metadata[model.MemUsageKey] = strconv.FormatFloat(memUsage+0.05, 'f', 2, 64)
      worker.Metadata[model.GoGoroutineKey] = strconv.FormatFloat(goroutineNum+1, 'f', 2, 64)

      s.updateWorkerInCache(worker)
  }

  问题:
  - 多个goroutine可能同时调用updateLocalResourceEstimate修改同一个worker
  - 对worker.Metadata的修改没有原子性保护
  - 可能导致资源统计不准确

  3. updateWorkerInCache的潜在问题 (第338-347行)

  func (s *Scheduler) updateWorkerInCache(updatedWorker discover.Instance) {
      workers := s.getAvailableWorkers()  // 创建副本
      for i, worker := range workers {
          if worker.ID() == updatedWorker.ID() {
              workers[i] = updatedWorker  // 修改副本
              break
          }
      }
      s.setAvailableWorkers(workers)  // 替换整个slice
  }

  问题:
  - 虽然通过atomic.Value的Store/Load提供了基本的原子性
  - 但在多个goroutine同时更新不同worker时，可能存在更新丢失的问题
  - 最后一个setAvailableWorkers调用会覆盖之前的更新

  4. watchWorkers中的不一致状态 (第221-244行)

  func (s *Scheduler) watchWorkers() error {
      return s.discover.Subscribe(
          func(services []discover.Instance, err error) {
              // ... 处理services ...

              if s.hasAvailableWorkersChanged(newAvailableWorkers) {
                  s.rwmu.Lock()
                  s.setAvailableWorkers(newAvailableWorkers)  // 在锁内更新
                  s.rwmu.Unlock()

                  s.triggerReAssignEvent()
              } else {
                  s.setAvailableWorkers(newAvailableWorkers)  // ❌ 在锁外更新
              }
          },
      )
  }

  问题:
  - 当worker列表没有变化时，setAvailableWorkers在锁外执行
  - 这可能导致与selectWorkerID中的读操作产生数据竞争

  具体风险场景

  场景1: 资源估算错误累积

  // 两个goroutine同时选择同一个worker
  Goroutine1: s.updateLocalResourceEstimate(worker) // CPU: 0.1 -> 0.15
  Goroutine2: s.updateLocalResourceEstimate(worker) // CPU: 0.1 -> 0.15 (读取了旧值)
  // 结果：CPU应该是0.2，但实际只增加了0.15

  场景2: Worker缓存更新丢失

  // 两个goroutine同时更新不同worker的缓存
  Goroutine1: 读取workers -> 更新worker1 -> setAvailableWorkers(workersWithWorker1Updated)
  Goroutine2: 读取workers -> 更新worker2 -> setAvailableWorkers(workersWithWorker2Updated)
  // 结果：只有worker2的更新被保留，worker1的更新丢失

  建议的修复方案

  1. 修复selectWorkerID的竞态条件

  func (s *Scheduler) selectWorkerID(task *model.Task) (string, error) {
      s.rwmu.Lock()
      defer s.rwmu.Unlock()

      // ... 选择worker逻辑 ...

      // 在锁保护下更新资源估算
      s.updateLocalResourceEstimateUnsafe(selectedWorker)
      return selectedWorker.ID(), nil
  }

  2. 使用更细粒度的锁或原子操作

  type Scheduler struct {
      rwmu    sync.RWMutex
      workerMu sync.Mutex  // 专门保护worker metadata修改
      // ...
  }

  func (s *Scheduler) updateLocalResourceEstimate(worker discover.Instance) {
      s.workerMu.Lock()
      defer s.workerMu.Unlock()

      // 安全地更新metadata
      // ...
  }

  这些并发问题在高并发场景下会导致：
  - 负载均衡决策基于不准确的数据
  - 任务分配不均匀
  - 系统性能下降



